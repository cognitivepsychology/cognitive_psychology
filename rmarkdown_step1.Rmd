---
title: '한국어 구어 말뭉치를 대상으로 한 연어구성 추출 방법 비교: 1단계_말뭉치 원자료 선처리'
author: "Cognitive Psychology"
date: "2016년 1월 4일"
output:
  html_document:
    css: style.css
    highlight: haddock
    self_contained: no
    theme: journal
    toc: yes
csl: apa.csl
bibliography: Packages.bib
---
본 연구(투고중)는 한국어 구어 말뭉치를 바탕으로 연어구성을 추출할 때 사용되는 다양한 어휘간 결합도 측정 방법(lexical association measures, 이하 AM)의 
효율성을 평가하는 것을 목적으로 한다. 본 연구에서는 다섯 가지 AM --- 공기빈도, 상호정보(mutual information),
카이제곱(χ^2^) 검정(chi-square test), *t*-검정(*t*-test), 로그 우도비(log-likelihood ratio) --- 의 연어검증 효율성을 다각도로 평가하고자 한다. 
이를 위해 국외 연어구성 추출 연구에서 AM의 효율성을 평가하기 위해 널리 사용되고 있는 전략인 연어판별 정확도(precision)[^secondnote]와
연어 포함률(recall)[^thirdnote]을 도입하고자 한다. 한국어 구어의 특징을 반영한 실증적 연어연구가 절대적으로 부족한 현실을 감안하여 세종 현대 구어 형태 분석 말뭉치(국립국어원, 2007)의 목적어+서술어(이하 목+술) 연쇄를 분석대상으로 삼기로 한다. 

아울러 본 연구자는 한국어 구어 연어구성 추출에 관심 있는 후속 연구자들을 위해 자료, 자료의 처리, 분석,‧시각화에 이용된 코드와 부연설명 그리고 최종 
결과물 일체를 본 연구자의 [github 저장소](https://github.com/cognitivepsychology/cognitive_psychology)에 공개하고자 한다. 이렇듯 연구의 자원, 절차,
결과를 투명하게 공개할 경우, 다른 연구자들이 동일한 자료를 대상으로 본 연구의 절차를 적용했을 때 동일한 결과를 재현할 수 있게 된다. 그리고 이는 본
연구의 내적, 외적 타당성을 검증하는 데도 매우 효과적이다(Gandrund, 2013). 공개된 코드는 목적어+서술어뿐만 아니라 주어+서술어 또는 부사+용언 등 다른
통사구성을 지닌 연어구성을 추출하고, 각 추출방법의 효율성을 평가하는 데도 손쉽게 응용할 수 있다. 본 연구의 연구문제는 다음과 같다.

> 한국어 구어 말뭉치에 나타난 목적어+서술어 연쇄를 대상으로 공기빈도, 상호정보, 카이제곱 검정, t-검정, 로그 우도비를 사용하여 연어구성을 추출할 경우, 각 어휘간 결합도 측정 방법의 연어판별 정확도와 연어 포함률은 어떤 양상을 나타내는가?

본 연구의 절차는 크게 세 단계로 구분되는데, 1단계는 말뭉치 원자료 선처리, 2단계는 말뭉치 자료 분석 그리고 3단계는 말뭉치 자료 분석 
결과의 시각화를 다룬다. 본 안내문은 그 가운데 1단계, 즉 연어판별 정확도와 연어 포함률을 분석할 수 있는 형태로 말뭉치 원자료를 가공하는
일련의 과정을 소개한다.

# 1. 말뭉치 원자료 파일을 분석 가능한 형식으로 변환하기

본 연구의 원자료인 세종 현대 구어 형태 분석 말뭉치(국립국어원, 2007) 파일은 아래한글로만 읽어들일 수 있는 hwp 형식으로 되어 있다. 
따라서 R로 읽어들여 작업할 수 있는 txt 형식으로 변환해야 한다. 세종 구어 말뭉치 파일은 
[국립국어원 웹사이트](https://ithub.korean.go.kr/user/corpus/referenceManager.do)에 가입만 하면 누구든
무료로 내려받을 수 있다. 파일 형식을 hwp에서 txt로 자동 변환해주는 프로그램은 [여기](http://skylee743.blog.me/112110946)에서 
무료로 내려받을 수 있다. 본 연구에서는 연구 절차와 결과의 재현 가능성을 담보하기 위해 해당 말뭉치 파일(총 200개)의
txt 버전과 그 압축 파일을 본 연구자의 [github 저장소](https://github.com/cognitivepsychology/cognitive_psychology/tree/master/RawData)에 
올려두었다. 그러나 비저작권자로서 해당 말뭉치 자료 파일을 개인 웹사이트에 공개하는 것이 문제가 될 경우 즉시 삭제할 것임을 
알려둔다.

# 2. 말뭉치 원자료에 대한 전처리 작업 수행하기

## (1) 말뭉치 원자료 파일 내려받기

본 연구자는 말뭉치 원자료 파일을 압축된 형태와 압축되지 않은 형태 두 가지 버전으로 본 연구자의 github 저장소에 올려두었다.
이제 R로써 분석 가능한 구조로 원자료를 가공하기 위해 말뭉치 원자료 파일(압축된 zip 파일 버전)을 로컬 컴퓨터로 내려받을 차례다. 
이때 후속작업의 편의를 위해 가급적 워킹 디렉토리의 하위 폴더에 파일을 저장할 것을 권고한다.

```{r eval=F, echo=F}
setwd("D:/collocation")
```

```{r eval=F, echo=T}
# 압축된 말뭉치 원자료 파일의 url 주소 축약하기.
raw <- "https://raw.github.com/cognitivepsychology/cognitive_psychology/master/RawData/RawData.zip"
# 압축된 말뭉치 원자료 파일 내려받기(url이 https로 시작하는 사이트에 저장된 자료는 download.file("url", "파일명")으로 내려받지 못할 수도 있음. 따라서 https 사이트 자료도 내려받을 수 있게 해주는 패키지 httr을 불러올 것.
library(httr)
response <- GET(raw,
                write_disk("raw.zip"),
                progress())
# 압축된 말뭉치 원자료 파일을 RawData 폴더에 풀기.
unzip("raw.zip", exdir="RawData")
```

## (2) 말뭉치 원자료 파일 가공하기

말뭉치 원자료 파일의 저장을 마쳤다면, 모든 원자료 파일의 텍스트 라인(text lines)을 읽어들인 뒤 분석에 필요한 부분만 남기고
나머지는 모두 제거한다. 그리고 나서 해당 자료를 분석 가능한 연어 단위, 즉 목+술 연쇄(noun-verb bigrams)로 분절한다.
이를 위해 먼저 말뭉치 원자료 파일의 텍스트 라인을 읽어들인다.[^firstnote]

```{r eval=F, warning=F, echo=T}
# 모든 말뭉치 원자료 파일명에 대한 목록 작성하기.
whole <- list.files(path = "RawData/", pattern = "*CT*")
# 말뭉치 원자료 파일이 저장된 디렉토리를 워킹 디렉토리로 변경하기.
setwd("RawData/")
# 자료 변형 및 가공을 도와주는 패키지 plyr 불러오기.
library(plyr) 
# 목록 내 모든 말뭉치 원자료 파일의 텍스트 라인 읽어들이기.
list.whole <- llply(whole, readLines)
```

본 연구를 위한 분석에 포함되지 않는 부분을 제거한다. 가령, < > 안에 있는 부분은 일종의 지문으로, 말뭉치 담당 수집자의 논평에 해당한다. 
이렇듯 R을 사용한 분석에 필요하지 않거나 걸림돌이 될 만한 모든 요소를 제거한다.

```{r eval=F, warning=F, echo=T}
# 파일명이 들어가 있으면서 "<"를 지닌 부분을 ""으로 바꾸기. 
whole.1 <- llply(list.whole, function(x) gsub("[0-9]CT.*<", "", x)) 
# "<"가 없는 라인들만 추리기(해당 말뭉치 파일에 대한 앞부분의 소개글 제거).
whole.2 <- llply(whole.1, function(x) grep("[0-9]CT", x, value=T)) 
# 라인 번호와 형태소 분석 태그 없는 어절을 지우고, 태그된 어절만 남기기.
whole.3 <- llply(whole.2, function(x) gsub("[0-9]CT.*\t.*\t", "", x)) 
# 각종 punctuation 기호 제거(단, 사선 "/"만 남겨둠).
whole.4 <- llply(whole.3, function(x) gsub("[]$*+.?[^{|(\\#%&~_<=>'!,:;`\")}@-]", "", x))
# 말뭉치 원자료 파일에 반복적으로 나타나는 알파벳 "c" 없애기.
whole.5 <- llply(whole.4, function(x) gsub("c", "", x)) 
# 부호를 나타내는 태그(/SF, /SP, /SS, /SE, /SO, ./SW) 제거.
whole.6 <- llply(whole.5, function(x) gsub("/SF|/SP|/SS|/SE|/SO|/SW", "", x))
# 일차 가공이 완료된 200개의 자료 파일을 합쳐 하나의 R 텍스트 벡터로 만들기.
whole.7 <- paste(whole.6, collapse=" ")
```

일차 가공된 자료를 분석 가능한 연어 단위, 즉 목+술 연쇄로 분절할 차례다. 이를 위해 우선 해당 자료를 두 단어 연쇄(bigrams)
단위로 분절한다.

```{r eval=F, warning=F, echo=T}
# 텍스트 마이닝 패키지 stylo 불러오기.
library(stylo) 
# 텍스트의 단위를 행(line)에서 어절(word)로 바꿔줌.
whole.words <- txt.to.words(whole.7, splitting.rule ="[ \t\n]+", preserve.case = T) 
# 잡다한 punctuation 기호 없애기(단, "/"는 남겨둠).
whole.words.1<- gsub("[]$*+.?[^{|(\\#%&~_<=>'!,:;`\")}@-]", "", whole.words) 
# 일차 가공된 자료를 두 단어 연쇄 단위로 분절함.
whole.2gram <- make.ngrams(whole.words.1, ngram.size = 2)
# 일차 가공된 자료를 세 단어 연쇄로 분절함. 이는 목적어+부사어+서술어 연쇄를 가려내기 위해 사용될 것임.
whole.3gram <- make.ngrams(whole.words.1, ngram.size = 3)
```

두 단어 연쇄 단위로 변환된 자료에서 목+술 연쇄만 추려낸 뒤, 분석의 편의를 위해 목적격 조사를 "을/JKO"로 통일한다.

```{r eval=F, warning=F, echo=T}
# JKO(목적격 조사 표지)와 VV(일반동사) 또는 JKO와 XSV(파생동사)로 구성된 두 단어 연쇄만 추리기.
whole.2gram.n.v <- grep("JKO[ \t\n]+[^a-zA-Z]+VV|JKO[ \t\n]+.+XSV", whole.2gram, value=T)
# R에 내장된 기본 텍스트 마이닝 명령어인 grep, gsub의 기능이 확장된 텍스트 마이닝 패키지 stringr 불러오기.
library(stringr) 
# 용언+명사형 전성어미/종결어미 구성(총 286개)이 앞에 오는 경우만 제외하기.
whole.2gram.n.v <- grep("^+.+/ETN+.+[ \t\n\r\f\v]|^+.+/EF+.+[ \t\n\r\f\v]", whole.2gram.n.v, value = T, invert = T) 
# VV 어간만 남기고, 분석의 편의를 위해 그 뒤에 마침표를 붙이기.
whole.2gram.n.v.root <- str_replace_all(whole.2gram.n.v, c("VV+.+."), "VV.")
# XSV 어간만 남기고, 분석의 편의를 위해 그 뒤에 마침표를 붙이기.
whole.2gram.n.v.root.1 <- str_replace_all(whole.2gram.n.v.root, c("XSV+.+."), "XSV.")  
# 분석의 편의를 위해 "ㄹ, 을, 를/JKO"를" "을/JKO"로 통일하기.
whole.2gram.n.v.root.2 <- str_replace_all(whole.2gram.n.v.root.1, "를/JKO", "을/JKO")
whole.2gram.n.v.root.2 <- str_replace_all(whole.2gram.n.v.root.2, "ㄹ/JKO", "을/JKO")
# 목적어 + 서술어 연쇄 목록 벡터.
whole.2gram.n.v.root.2
```

세 단어 연쇄 단위로 변환된 자료에서 목적어+부사어+서술어 연쇄(이하 목+부+술 연쇄)만 추려낸 뒤, 부사어만 삭제하여 목+술 연쇄로 만든다.

```{r eval=F, warning=F, echo=T}
# 세 단어 연쇄 자료에서 목적어(JKO) + 부사어[형용사(vA)+연결어미/부사(MAG)] + 서술어 연쇄 찾아내기.
whole.3gram.n.a.v <- grep("JKO[ \t\n]+[^a-zA-Z]+VA+.+VV|JKO[ \t\n]+[^a-zA-Z]+VA+.+XSV|JKO[ \t\n]+[^a-zA-Z]+MAG+.+VV|JKO[ \t\n]+[^a-zA-Z]+MAG+.+XSV", whole.3gram, value=T) 
# 부사(MAG)+동사 파생 접미사(XSV) 결합된 세 단어 연쇄를 목록에서 제거하기(이 구성은 이미 두 단어 연쇄 목록에 있음).
whole.3gram.n.a.v.1 <- gsub("[^a-zA-Z]/MAG+[^a-zA-Z]/XSV", "", whole.3gram.n.a.v)
# 세 단어 연쇄 자료에서 목+부+술 연쇄만 추리기.
whole.3gram.n.a.v.2 <- grep("JKO[ \t\n]+[^a-zA-Z]/VA+.+/VV|JKO[ \t\n]+[^a-zA-Z]/VA+.+/XSV|JKO[ \t\n]+[^a-zA-Z]/MAG+.+/VV|JKO[ \t\n]+[^a-zA-Z]/MAG+.+/XSV", whole.3gram.n.a.v.1, value=T) 
# 부사(MAG)를 제거하여 목+부+술 연쇄를 목+술 연쇄로 만들기.
whole.3gram.n.a.v.3 <- str_replace_all(whole.3gram.n.a.v.2, "[^a-zA-Z]/MAG", "")
# 부사어(형용사+연결어미)를 제거하여 목+부+술 연쇄를 목+술 연쇄로 만들기.
whole.3gram.n.a.v.4 <- str_replace_all(whole.3gram.n.a.v.3, "[^a-zA-Z]/VA+.+[ \t\n\r\f\v]", " ")  
# 스페이스 두 개를 한 개로 줄이기.
whole.3gram.n.a.v.4 <- str_replace_all(whole.3gram.n.a.v.4, "  ", " ")
# 용언+명사형 전성어미/종결어미 구성(총 12개)이 앞에 오는 경우만 제외하기.
whole.3gram.n.a.v.5 <- grep("^+.+/ETN+.+[ \t\n\r\f\v]|^+.+/EF+.+[ \t\n\r\f\v]", whole.3gram.n.a.v.4, value =T, invert = T) 
# VV 어간만 남기고, 분석의 편의를 위해 그 뒤에 마침표를 붙이기.
whole.3gram.n.a.v.6 <- str_replace_all(whole.3gram.n.a.v.5, c("VV+.+."), "VV.")
# XSV 어간만 남기고, 분석의 편의를 위해 그 뒤에 마침표를 붙이기.
whole.3gram.n.a.v.7 <- str_replace_all(whole.3gram.n.a.v.6, c("XSV+.+."), "XSV.")  
# "ㄹ, 을, 를/JKO"을" "을/JKO"로 통일하기.
whole.3gram.n.a.v.8 <- str_replace_all(whole.3gram.n.a.v.7, "를/JKO", "을/JKO")
whole.3gram.n.a.v.8 <- str_replace_all(whole.3gram.n.a.v.8, "ㄹ/JKO", "을/JKO")
# 목+부+술 연쇄들 가운데서 추린 목+술 연쇄 목록 벡터.
whole.3gram.n.a.v.8 
```

두 단어 연쇄 자료에서 찾은 목+술 연쇄와 세 단어 연쇄 자료에서 걸러낸 목+술 연쇄를 한 데 합친다. 그런 뒤 이 목+술 연쇄 목록을 "목적어, 서술어, 목+술 연쇄"가 
하나의 행(row)을 이루는 데이터프레임으로 변환한다. R을 사용하여 통계분석을 수행하고자 할 경우, 반드시 자료가 데이터프레임 형식으로 되어 있어야 한다.

```{r eval=F, warning=F, echo=T}
# 두 단어 연쇄와 세 단어 연쇄 목록 벡터를 합쳐 최종 목+술 연쇄 목록 벡터 완성하기.
whole.nv <- append(whole.2gram.n.v.root.2, whole.3gram.n.a.v.8) 
# 연어구성이 될 수 없는 고유명사(NNP)/수사(NNB)/의존명사(NR)/대명사(NP) + 서술어 구성 등을 제외한 일반명사(NNG) + 동사(VV) 구성만 추출하기.
whole.nv0 <- grep("NNG+.*을", whole.nv, value = T) 
# 분석에 불필요한 "/"(슬래시) 없애기.
whole.nv1 <- str_replace_all(whole.nv0, "[]$*+.?[^{|(\\#%&~_/<=>'!,:;`\")}@-]", "") 
# 입력의 편의를 위해 벡터명을 짧은 것으로 바꾸기.
x <- whole.nv1 
# 목적어(word1), 서술어(word2), 목+술 연쇄(word1+word2)가 하나의 행을 이루는 데이터프레임 만들기.
whole.nv2 <- data.frame(cbind(do.call('rbind', strsplit(x, " ")), x)) 
# 열 제목을 "word1", "word2", "bigram"으로 바꾸기.
colnames(whole.nv2) <- c("word1", "word2", "bigram")
```


# 3. 가공된 말뭉치 자료에 분석을 위한 각종 양적 변수 추가하기


## (1) 가공된 말뭉치 자료 토대로 빈도 항목 추가하기

각각의 목적어, 서술어 그리고 목+술 연쇄의 빈도를 계산한 뒤, 이들 세 항목의 빈도를 별도의 열로 추가한다. 
이들 항목의 빈도자료는 앞으로 이어질 분석의 토대가 된다. 

```{r eval=F, warning=F, echo=T}
# 데이터프레임 자료를 좀더 간편하게 가공할 수 있도록 도와주는 패키지인 data.table 패키지 불러오기.
library(data.table) 
# words & 2-gram 자료 whole.nv3를 데이터테이블 형식으로 변환하기.
whole.nv2.dt <- data.table(whole.nv2)
# 목적어(word1), 서술어(word2), 목+술 연쇄(bigram), 목적어(word1) 빈도, 서술어(word2) 빈도, 목+술 연쇄(bigram) 빈도 열로 구성된 데이터프레임 완성하기.
a <- whole.nv2.dt[, freq.bi := .N, by=bigram] 
b <- whole.nv2.dt[, freq.w1 := .N, by=word1]
c <- whole.nv2.dt[, freq.w2 := .N, by=word2]
d <- data.frame(c)
e <- d[, c(1, 2, 3, 5, 6, 4)] 
# bigram 열 제목 "freq"를" "freq.bi"로 수정하기.
colnames(e)[6] <- "freq.bi"
# 분석의 편의를 위해 데이터프레임 이름을 whole.nv.df로 변경하기.
whole.nv.df <- e 
```

분석에 혼란을 줄 수 있으므로 중복되는 행은 제거한다. 나중에 엑셀에서 좀더 쉽고 편하게 자료를 살펴볼 수 있도록, 
항목별 빈도가 기록된 데이터프레임 자료를 저장하도록 한다.

```{r eval=F, warning=F, echo=T}
# 중복되는 행 제거하기.
whole.nv.df.uni <- unique(whole.nv.df)
# 중복자료 제거 전 데이터 행 수(N = 19652).
nrow(whole.nv.df)
# 중복자료 제거 후 데이터 행 수(N = 9342).
nrow(whole.nv.df.uni) 
# 입력의 편의를 위해 데이터프레임 이름 수정하기.
whole.uni <- whole.nv.df.uni 
```

목(W1)+술(W2) 연쇄들의 어휘 결합도(특히 카이제곱 값, t-값, 로그 우도비)를 측정하려면 반드시 빈도 분할표(contingency table)를 작성해야 한다.
이를 위해 먼저 어휘 결합도 측정에 필요한 관찰빈도인 O11, O12, O21, O22, R1, R2, C1, C2, N을 계산한다. 이들 수치의 계산공식은 다음과 같다.

```{r eval=T, warning=F, echo=F, results='asis'}
fstcol <- c("", "W1","W1을 제외한 모든 단어", "") 
sndcol <- c("W2", "O11", "O21", "O11 + O21= C1")
trdcol <- c("W2를 제외한 모든 단어", "O12", "O22", "O12 + O22 = C2")
fthcol <- c("", "O11 + O12 = R1", "O21 + O22 = R2", "O11 + O12 + O21 + O22 = N")
contable <- data.frame(fstcol, sndcol, trdcol, fthcol)
conkable <- contable[2:4, 2:4]
colnames(conkable) <- c("W2", "W2를 제외한 모든 단어", "")
rownames(conkable) <- c("W1", "W1을 제외한 모든 단어", "")
library("knitr")
kable(conkable, caption = "<표 1> 관찰빈도 분할표")
```  

이제 본격적으로 어휘 결합도 측정에 필요한 수치인 R1, R2, C1, C2, N을 계산할 차례다. 이때 본 연구에서는 공기빈도가 2회 이상인 목+술 연쇄만 분석대상으로
삼는바, 공기빈도가 2회 미만인 목+술 연쇄는 제거하기로 한다.

```{r eval=F, warning=F, echo=T}
# 공기빈도가 2회 이상인 자료만 남기고, 나머지는 NA(not avaible)로 처리하기.
test1 <- ifelse(whole.uni$freq.bi > 1, whole.uni$freq.bi, NA) 
whole.uni.test1 <- cbind(whole.uni, test1)
# NA가 있는 행(공기빈도 2회 미만인 목+술 연쇄) 제거하기.
whole.uni.one <- na.omit(whole.uni.test1) 
# 행 수(두 단어 연쇄의 수) 확인(N = 2340).
nrow(whole.uni.one)
# bigram열과 중복되는 test1열 지우기.
whole.one <- whole.uni.one[, 1:6] 
```

어휘 결합도 측정에 필요한 수치인 관찰빈도 O11, O12, O21, O22, R1, R2, C1, C2, N을 위한 빈도 분할표 열을 생성한다.

```{r eval=F, warning=F, echo=T}
# o11열(특정 목+술 연쇄 빈도) 생성하기.
o11 <- whole.one[, 6] 
# o12열(w1 - o11 빈도) 생성하기.
o12 <- whole.one[, 4] - whole.one[, 6] 
# o21열(w2 - o11 빈도) 생성하기.
o21 <- whole.one[, 5] - whole.one[, 6] 
# o22열(전체 목+술 구조 출현형[token] - w1 - w2 + o11 빈도) 생성하기.
o22 <- nrow(whole.nv.df) - whole.one[, 4] - whole.one[, 5] + whole.one[, 6] 
# o11, o12, o21, o22 열들 하나로 합치기.
whole.one.ct <- cbind(whole.one, o11, o12, o21, o22)
# R1, R2, C1, C2, N 열 생성하기.
whole.one.ct.add <- transform(whole.one.ct, r1=o11+o12, r2=o21+o22, c1=o11+o21, c2=o12+o22, n=o11+o12+o21+o22)
```

어휘 결합도 측정에 필요한 수치인 기대빈도 E11, E12, E21, E22를 위한 빈도 분할표 열을 생성한다. 이들 수치의 계산공식은 다음과 같다.

```{r eval=T, warning=F, echo=F, results='asis'}
fstcol0 <- c("E11", "E21") 
sndcol0 <- c("E12", "E22")
contable0 <- data.frame(fstcol0, sndcol0)
colnames(contable0) <- c("W2", "W2를 제외한 모든 단어")
rownames(contable0) <- c("W1", "W1을 제외한 모든 단어")
kable(contable0, caption = "<표 2> 기대빈도 분할표")
```  

* \[E11 = \frac{R1C1}{N}\]
* \[E12 = \frac{R1C2}{N}\]
* \[E21 = \frac{R2C1}{N}\]
* \[E22 = \frac{R2C2}{N}\]

```{r eval=F, warning=F, echo=T}
# E11, E12, E21, E22 열 생성하기.
whole.one.ct.add1 <- transform(whole.one.ct.add, e11=(r1*c1)/n, e12=(r1*c2)/n, e21=(r2*c1)/n, e22=(r2*c2)/n)
# 주의: 모든 열의 특성을 numeric으로 바꾸기(큰 값의 integer를 가지고 계산할 경우 integer overflow 현상이 일어남).
as.numeric(whole.one.ct.add1[, 4])
as.numeric(whole.one.ct.add1[, 5])
as.numeric(whole.one.ct.add1[, 6])
as.numeric(whole.one.ct.add1[, 7])
as.numeric(whole.one.ct.add1[, 8])
as.numeric(whole.one.ct.add1[, 9])
as.numeric(whole.one.ct.add1[, 10])
as.numeric(whole.one.ct.add1[, 11])
as.numeric(whole.one.ct.add1[, 12])
as.numeric(whole.one.ct.add1[, 13])
as.numeric(whole.one.ct.add1[, 14])
as.numeric(whole.one.ct.add1[, 15])
as.numeric(whole.one.ct.add1[, 16])
as.numeric(whole.one.ct.add1[, 17])
as.numeric(whole.one.ct.add1[, 18])
as.numeric(whole.one.ct.add1[, 19])
```

## (2) AM별 측정치 항목 추가하기

개별 목+술 연쇄의 관찰빈도와 기대빈도 자료를 토대로, 네 가지 AM --- 카이제곱 검정, 로그 우도비, *t*-검정, 상호정보 --- 의 
측정치를 구한다. 개별 목+술 연쇄의 공기빈도는 이미 "bigram"(또는 "o11")이라는 열로 정리된 상태이므로 다시 구할 필요는 없다.

### 1) 카이제곱 값 계산 및 측정치 열 생성하기
```{r eval=F, warning=F, echo=T}
# R1*R2*C1*C2 값이 커서 integer overflow 메시지가 뜰 수 있으므로, R1*R2와 C1*C2를 나누어 계산할 것.
whole.one.ct.add.chi <- transform(whole.one.ct.add1, chisq = n * 
                                    (abs(o11*o22-o12*o21) - n/2)^2 / 
                                    (r1*r2)) # r1*r2를 먼저 나눠줌.
whole.one.ct.add.chisquare <- transform(whole.one.ct.add.chi, chisquare = chisq / (c1*c2)) # C1*C2를 나중에 나눠줌.
# R1*R2를 분모로 사용한 chisq 열을 지우고, 진짜 카이제곱 공식으로 구한 카이제곱 값 열만 남기기.
whole.one.ct.add.chisq <- whole.one.ct.add.chisquare[names(whole.one.ct.add.chisquare) !="chisq"]
# 분석의 편의를 위해 열 제목을 chisq로 바꾸기.
colnames(whole.one.ct.add.chisq)[20] <- "chisq"
```

카이제곱 검정은 기대빈도가 5 미만인 빈도 분할표상 항목(본 연구에서는 E11, E12, E21, E22)의 수가 전체의 20퍼센트를 넘을 경우 결과의 정확도가 떨어진다는 단점이 있다.
따라서 본격적 분석에 앞서 E11, E12, E21, E22의 기대빈도를 직접 확인해볼 필요가 있다.

```{r eval=F, warning=F, echo=T}
sum(whole.one.ct.add.chisq$e11 < 5) / 2340 # 0.9465812
sum(whole.one.ct.add.chisq$e12 < 5) / 2340 # 0.4235043
sum(whole.one.ct.add.chisq$e21 < 5) / 2340 # 0.06794872
sum(whole.one.ct.add.chisq$e22 < 5) / 2340 # 0
```

### 2) 로그 우도비 계산 및 측정치 열 생성하기

```{r eval=F, warning=F, echo=T}
whole.one.ct.add.log <- transform(whole.one.ct.add.chisq,
                                  logl = 2 * (
                                  ifelse(o11>0, o11*log(o11/e11), 0) +
                                  ifelse(o12>0, o12*log(o12/e12), 0) +
                                  ifelse(o21>0, o21*log(o21/e21), 0) +
                                  ifelse(o22>0, o22*log(o22/e22), 0)
                                  )) # log 0은 계산불가임. 따라서 관찰빈도가 0인 경우, 로그 우도비 값을 0으로 처리.

```

### 3) *t*-값 계산 및 측정치 열 생성하기

```{r eval=F, warning=F, echo=T}
whole.one.ct.add.ttest <- transform(whole.one.ct.add.log, ttest = (o11-e11) / sqrt(o11))
```

### 4) 상호정보 계산 및 측정치 열 생성하기

```{r eval=F, warning=F, echo=T}
whole.one.ct.add.mi <- transform(whole.one.ct.add.ttest, MI = log2(o11/e11))
```

## (3) 전문가 판정 결과 항목 추가하기

국어 전문가 두 명이 빈도 2회 이상 목+술 연쇄 2340개에 대해 직접 연어구성 여부를 판정한 결과를
"연어가 맞음 = TRUE", "연어가 아님 = FALSE"로 정리하여 불린(boolean) 벡터로 정리해두었다. 이 전문가
판정 결과와 다섯 가지 어휘 결합도 측정 방법의 측정치를 비교함으로써 이들 측정방법의 연어검증
효율성을 평가하게 된다. 어휘 결합도 측정 방법들의 연어검증 효율성을 비교, 평가하는 데 사용되는
전략에 대해서는 "한국어 구어 말뭉치를 대상으로 한 연어구성 추출 방법 비교: 2단계_말뭉치 자료 분석"
문서를 참조하기 바란다.

```{r eval=F, warning=F, echo=T}
# 전문가의 연어관계 판정 결과가 담긴 파일 R로 불러들이기.
whole.one.human <- read.csv(file="https://raw.github.com/cognitivepsychology/cognitive_psychology/master/whole.one.human.csv")
# 2340개의 TRUE 또는 FALSE로 구성된 "human"이라는 제목의 전문가 판정 결과 열을 기존 빈도자료 데이터프레임에 추가하기.
human <- whole.one.human$human.eval 
whole.one.am.human <- transform(whole.one.ct.add.mi, human = human) 
```

```{r eval=T, warning=F, echo=F}
library(knitr)
write_bib(c("plyr", "stylo", "stringr", "data.table"), file = "Packages.bib")
```

[^secondnote]: 특정 AM을 사용하여 얻은 수치 순으로 정렬한 연어구성 후보 상위 *n*개 목록 중에서 진짜 연어구성이 차지하는 
비율을 가리킨다. 측정치 상위권 목록의 정확도가 높을수록 해당 측정방법이 주어진 어휘연쇄들의 연어관계를 정확히 포착해낼 가능성이 높음을 의미한다. 
연어판별 정확도 산출 공식은 [2단계 안내문](https://github.com/cognitivepsychology/cognitive_psychology/blob/master/rmarkdown_step2.Rmd)을 참조하라.
[^thirdnote]: 연어 포함률이란 특정 AM을 사용하여 얻은 수치 순으로 정렬한 연어구성 후보 상위 *n*개 목록 안에 전체 진짜 연어구성 
중 몇 퍼센트의 진짜 연어구성이 포함되어 있는가를 가리킨다. 측정치 상위권 목록의 포함률이 높을수록 해당 측정방법이 주어진 어휘연쇄들의 연어관계를
정확히 그리고 효율적으로 포착해낼 가능성이 높음을 의미한다. 연어 포함률 산출 공식은 [2단계 안내문](https://github.com/cognitivepsychology/cognitive_psychology/blob/master/rmarkdown_step2.Rmd)을 참조하라.
[^firstnote]: 본 연구의 말뭉치 원자료 선처리 작업에 사용된 R 패키지 목록은 다음과 같다: data.table[@R-data.table], plyr[@R-plyr],
stringr[@R-stringr], stylo[@R-stylo].

# References

국립국어원. (2007). **21세기 세종계획 최종 성과물.** 서울: 국립국어원.

Gandrud, C. (2013). *Reproducible research with R and R Studio.* Boca Raton, FL: CRC Press.

**<인터넷 자료>**
